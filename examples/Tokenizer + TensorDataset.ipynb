{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O objetivo do notebook é criar pipelines de dados que lidem com generators (situação bastante comum com grandes datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_classes):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_channels=42, out_channels=30, kernel_size=4)\n",
    "        self.max_pool = nn.MaxPool1d(kernel_size=4)\n",
    "        self.dense_1 = nn.Linear(in_features=3, out_features=1024)\n",
    "        self.dense_2 = nn.Linear(in_features=1024, out_features=512)\n",
    "        self.dense_3 = nn.Linear(in_features=512, out_features=256)\n",
    "        self.dense_4 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.dense_5 = nn.Linear(in_features=128, out_features=num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        net = self.conv(x)\n",
    "        net = self.max_pool(net)\n",
    "        net = net.unsqueeze(1)\n",
    "        net = F.relu(self.dense_1(net))\n",
    "        net = F.relu(self.dense_2(net))\n",
    "        net = F.relu(self.dense_3(net))\n",
    "        net = F.relu(self.dense_4(net))\n",
    "        net = self.dense_5(net)\n",
    "        net = net.squeeze(1)        \n",
    "        net = net.mean(1)        \n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding,\n",
    "                 estimator):\n",
    "        super().__init__()\n",
    "        self.embed = embedding\n",
    "        self.estimator = estimator\n",
    "        \n",
    "    def forward(self, x):\n",
    "        net = self.embed(x)\n",
    "        net = self.estimator(net)       \n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "        Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    max_preds = preds.argmax(dim=1, keepdim=True) # get the index of the max probability\n",
    "    correct = max_preds.squeeze(1).eq(y)\n",
    "    return correct.sum() / torch.FloatTensor([y.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model,\n",
    "        data,\n",
    "        optimizer,\n",
    "        criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    for step in range(100):        \n",
    "        for batch_x, batch_y in data:\n",
    "            predictions = model(batch_x)\n",
    "                       \n",
    "            loss = criterion(predictions, batch_y)\n",
    "            loss.backward(retain_graph=True)              \n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()   \n",
    "            \n",
    "            acc = categorical_accuracy(predictions, batch_y)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "                \n",
    "    return epoch_loss / 100, epoch_acc / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,\n",
    "             data,\n",
    "             criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0 \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for step in range(100):            \n",
    "            for batch_x, batch_y in data:\n",
    "                predictions = model(batch_x)\n",
    "                                           \n",
    "                loss = criterion(predictions, batch_y)\n",
    "                acc = categorical_accuracy(predictions, batch_y)\n",
    "            \n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc.item()\n",
    "    return epoch_loss / 100, epoch_acc / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('tabular_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(dataset.query_string.apply(lambda x: len(x.split(\" \"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [data for data in dataset['query_string']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_labels = sorted(dataset.output.apply(lambda x: x.split('/')[0]).unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = {v: k for k, v in enumerate(lookup_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(char_level=True,\n",
    "                      to_lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.build_vocab(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_generator = pd.read_csv('data.csv',\n",
    "                                usecols=['query_string', 'output'],\n",
    "                                chunksize=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(data_generator=dataset_generator,\n",
    "                  tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, val = dataset.split(batch_size=256,\n",
    "                                 max_len=max_len,\n",
    "                                 input_dim=42,\n",
    "                                 lookup_labels=lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(num_embeddings=42,\n",
    "                         embedding_dim=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Estimator(num_classes=47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network(embedding=embedding,\n",
    "                estimator=estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing training and validation\n",
      "Epoch: 01\n",
      "\tTrain Loss: 3.294 | Train Acc: 12.66%\n",
      "\t Val. Loss: 3.169 |  Val. Acc: 16.38%\n",
      "Epoch: 02\n",
      "\tTrain Loss: 3.075 | Train Acc: 19.86%\n",
      "\t Val. Loss: 3.005 |  Val. Acc: 20.58%\n",
      "Epoch: 03\n",
      "\tTrain Loss: 2.940 | Train Acc: 22.23%\n",
      "\t Val. Loss: 2.872 |  Val. Acc: 23.60%\n",
      "Epoch: 04\n",
      "\tTrain Loss: 2.787 | Train Acc: 26.19%\n",
      "\t Val. Loss: 2.728 |  Val. Acc: 26.99%\n",
      "Epoch: 05\n",
      "\tTrain Loss: 2.723 | Train Acc: 28.13%\n",
      "\t Val. Loss: 2.697 |  Val. Acc: 28.61%\n",
      "Epoch: 06\n",
      "\tTrain Loss: 2.668 | Train Acc: 29.35%\n",
      "\t Val. Loss: 2.626 |  Val. Acc: 30.48%\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing training and validation\")\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc  = fit(model, train, optimizer, criterion)\n",
    "\n",
    "    valid_loss, valid_acc = evaluate(model, val, criterion)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
